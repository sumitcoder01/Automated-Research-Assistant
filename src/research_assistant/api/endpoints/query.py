from fastapi import APIRouter, Depends, HTTPException, status
from research_assistant.schemas.query import QueryRequest, QueryResponse
from research_assistant.memory import ChromaSessionStore, session_store
from research_assistant.api.deps import get_session_store
from research_assistant.assistant.workflow import graph_app # Import compiled graph
from research_assistant.assistant.graph.state import GraphState
from langchain_core.messages import HumanMessage, AIMessage
import logging
import json

router = APIRouter(tags=["Research Query"])
logger = logging.getLogger(__name__)

@router.post("", response_model=QueryResponse) # Use "" for path relative to prefix
async def handle_query(
    request: QueryRequest,
    store: ChromaSessionStore = Depends(get_session_store) # Use DI for store
):
    """
    Handles a user query, orchestrates agents via LangGraph, and returns the response.
    Uses the specified provider for embeddings when adding messages.
    """
    session_id = request.session_id
    user_query = request.query
    # Determine the embedding provider based on the llm_provider field
    # Or use request.embedding_provider if you added that field to the schema
    embedding_provider_for_session = request.llm_provider or "google" # Default to google

    logger.info(f"Received query for session {session_id} (LLM: {request.llm_provider}, Embeddings: {embedding_provider_for_session}): '{user_query[:50]}...'")

    try:
        # 1. Retrieve history (optional)
        # history = store.get_history(session_id, limit=10)

        # 2. Add user message to store, PASSING THE PROVIDER HINT
        user_message = HumanMessage(content=user_query)
        store.add_message(
            session_id,
            user_message,
            embedding_provider=embedding_provider_for_session
        )

        # 3. Prepare initial state for the graph
        initial_state: GraphState = {
            "query": user_query,
            "session_id": session_id,
            "llm_provider": request.llm_provider or "openai", # Default LLM provider if needed
            "llm_model": request.llm_model,
            "messages": [user_message],
            # ... initialize other state fields ...
            "search_results": None,
            "summary": None,
            "category": None,
            "analysis": None,
            "report": None,
            "error": None,
            "next_node": None,
        }

        # 4. Invoke the LangGraph workflow
        graph_config = {"configurable": {"session_id": session_id}}
        final_state = await graph_app.ainvoke(initial_state, config=graph_config)

        if not isinstance(final_state, dict):
             logger.error(f"Graph execution returned unexpected type: {type(final_state)}")
             raise HTTPException(status_code=500, detail="Internal error during graph execution")

        logger.info(f"Graph execution completed for session {session_id}.")

        # 5. Determine the final response from the state
        response_content = "Processing complete, but no final response was generated." # Default
        if final_state.get("error"):
            # Prioritize showing errors that might have occurred during generation
            response_content = f"An error occurred: {final_state['error']}"
        elif final_state.get("final_response"):
            # Get the response generated by either direct or synthesis node
            response_content = final_state["final_response"]
        elif final_state.get("summary"): # Fallback if synthesis failed but summary exists
            response_content = "Summary generated:\n" + final_state["summary"]
        elif final_state.get("search_results"): # Fallback if only search happened
            valid_results = [r for r in final_state['search_results'] if isinstance(r,dict) and 'error' not in r]
            if valid_results:
                response_content = f"Search completed with {len(valid_results)} results. Synthesis step may have failed."
            else:
                response_content = "Search was attempted but yielded no valid results or failed."


        # 6. Add assistant response to store
        assistant_message = AIMessage(content=response_content)
        store.add_message(
            session_id,
            assistant_message,
            embedding_provider=embedding_provider_for_session
        )

        # 7. Return response
        return QueryResponse(
            session_id=session_id,
            query=user_query,
            response=response_content, # Return the extracted final_response
        )

    except ValueError as ve: # Catch config errors like missing API keys
         logger.error(f"Configuration error handling query for session {session_id}: {ve}")
         raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=f"Configuration error: {ve}")
    except Exception as e:
        logger.exception(f"Error handling query for session {session_id}: {e}")
        # Attempt to add error message to history (best effort)
        try:
             error_message = AIMessage(content=f"Sorry, an internal error occurred processing your request.")
             store.add_message(session_id, error_message, embedding_provider=embedding_provider_for_session)
        except Exception as store_e:
             logger.error(f"Failed to store error message for session {session_id}: {store_e}")

        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"An internal error occurred.")